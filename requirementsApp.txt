Work as 15 years experienced Senior Software engineer and create the production grade project with follwoing requirements. keep concurrent processing so it runs fast and write concise coding with comments. So it runs without a hick up, handle the exceptions as well so if there is any failure it logs it and move for the next one doesn't gets stopped.

Read all the following requirement and decide the tech stack first which would be the best choice for this project and let me know about that architecture as well

1) At first we need to keep one resume template need to read resume details. accordinng to that we need to scrape jobs.
2) now lets come to the job scrappping. All the scrapping will happen on the basis of the filtered data from the resume. We need to create multiple scrapping platform like Linkedin,naukri,indeed and so on in one place, which are currently giving best jobs in the market. 
3) All the scrapping will have some basic requirements those are following..
    a) There have to be multiple job roles that we're scrapping for.
    b) There will be multiple locations the scrapping will happen. Ex:- We've two arrays lets assume roles = [SDE,MERN,Backend Engineer] and Locations = [Remote, Hyderabad, Pune] then for locations priority will be Remote first then hyderabad then pune. for roles SDE will get searched in all locations MERN will get searched in all locations likewise. 
4) Now let's come the different scrappers :-
    a) For Big tech companies we need to read the csv where all carrers urls are listed we need to go to each url and search for matched jobs. no statict 2 to 5 companies. I've that csv you just link this thing with the code.
    
5) After scrapping all the job scrappers filter them on the basis of posted date first then matched results. Ex: - i've 100 jobs after scrapping with my resume keyworkds. Then i'll filter only last one week posted jobs or say today posted jobs then i'll get 70 jobs out of it then i filter again on the basis of resume match count which matches the most pick them lets say 50 jobs. Then filter the salary range( that also we need to store in code as some static value which i can change manually later on). then let's say we get 30 jobs These 30 jobs will get stored in db table FilteredJobs. And also keep a look in this table we don't get duplicate entries.

6) there will be a second table which will store only last one day posted jobs Named PostedToday. Lets say after filtering jobs and storing the data in FilteredJobs table of all day jobs it'll filter only one day jobs and Store it in PostedToday table, Afterwards this PostedToday table will get cleared every day.

7) Keep open for future requirements :-
    A) In the future we'll create one auto apply option as well. which will apply on behalf of the applicant. 
    B) Add some email functionalities as well which send an email afer srapping the best jobs and after applying on them that how much application succeded or how much failed. 
    C) you can also suggest any good requirement which may goes by the applicant needs


Do a dry run atleast 10 times with mock data and check for missing variables imports all the connectivity to each component. If it is good then add the real live urls real scrapping with all live links and do simulations of atleast 30 times when a get a good responce and get positive results then return me the whole project each file by file no need to create a zip or push to repo return me all the files guided by path. I'll manually create the project from scratch.


For your refeference i'm sharing one script in python which i creaed and succeeded achieving almost every requirement i shared. So take a look into that as well. I did it in python this doesn't mean you've to choose python you understand the requirement and decide which would be best tech stack for this to implement. 




import re
import csv
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from collections import Counter
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from tqdm import tqdm
from bs4 import BeautifulSoup
from dateparser import parse
import concurrent.futures
import os
from threading import Semaphore
import logging
import random
from selenium.common.exceptions import *

# CONFIGURATION
RESUME_PATH = "resume.txt"
JOB_TITLES = ["software engineer"]
LOCATION_PRIORITY = ["Remote"]
COMPANY_LIST_PATH = "top_50_companies.csv"
MAX_LINKEDIN_PAGES = 1
MAX_COMPANY_JOBS = 3
OUTPUT_CSV = "daily_jobs.csv"
HISTORY_CSV = "job_history.csv"
CHROME_DRIVER_PATH = os.path.abspath("chromedriver")
MAX_JOBS = 30
MAX_WORKERS = 2
CHROME_SEMAPHORE = Semaphore(2)

# Initialize resume skills globally
RESUME_SKILLS = {}

# Chrome Options
chrome_options = Options()
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36")

# Dynamic Scraper Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

JOB_PATTERNS = {
    'job_card': [
        '//*[contains(translate(@class, "JOB", "job"), "job")]',
        '//*[contains(@id, "job")]',
        '//div[contains(.//text(), "opening") or contains(.//text(), "position")]',
        '//div[contains(@class, "JobCard")]',
        '//li[contains(@class, "job-item")]'
    ],
    'title': [
        './/h2|.//h3|.//h4',
        './/*[contains(@class, "title")]',
        './/*[contains(.//text(), "engineer") or contains(.//text(), "developer")]'
    ],
    'location': [
        './/*[contains(@class, "location")]',
        './/*[contains(.//text(), "location")]/following-sibling::*',
        './/span[contains(@class, "geo")]'
    ],
    'link': [
        './/a[contains(@href, "apply") or contains(@href, "job")]',
        './/*[contains(@class, "apply")]/ancestor::a'
    ]
}

class DynamicScraper:
    def __init__(self, driver):
        self.driver = driver
        self.retries = 3
        self.timeout = 30
        
    def random_delay(self):
        time.sleep(random.uniform(1.5, 4.0))
        
    def load_page(self, url):
        for attempt in range(self.retries):
            try:
                self.driver.get(url)
                self.driver.execute_script("return document.readyState") == "complete"
                return True
            except WebDriverException as e:
                logging.warning(f"Attempt {attempt+1} failed: {str(e)}")
                time.sleep(5)
        return False

    def find_jobs(self):
        for pattern in JOB_PATTERNS['job_card']:
            try:
                elements = self.driver.find_elements(By.XPATH, pattern)
                if elements:
                    logging.info(f"Found {len(elements)} jobs using pattern: {pattern}")
                    return elements
            except NoSuchElementException:
                continue
        return []

    def extract_details(self, element):
        job = {'title': 'N/A', 'location': 'N/A', 'url': 'N/A'}
        
        # Title
        for pattern in JOB_PATTERNS['title']:
            try:
                job['title'] = element.find_element(By.XPATH, pattern).text.strip()
                break
            except NoSuchElementException:
                continue

        # Location
        for pattern in JOB_PATTERNS['location']:
            try:
                job['location'] = element.find_element(By.XPATH, pattern).text.strip()
                break
            except NoSuchElementException:
                continue

        # URL
        for pattern in JOB_PATTERNS['link']:
            try:
                job['url'] = element.find_element(By.XPATH, pattern).get_attribute('href')
                break
            except NoSuchElementException:
                continue

        return job

    def scrape(self, url):
        if not self.load_page(url):
            return []
            
        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        self.random_delay()
        
        jobs = []
        elements = self.find_jobs()
        
        for element in elements:
            try:
                job = self.extract_details(element)
                if job['url'] and job['title'] != 'N/A':
                    jobs.append(job)
            except StaleElementReferenceException:
                continue
                
        return jobs

def extract_resume_skills():
    with open(RESUME_PATH, 'r') as f:
        cv_text = f.read().lower()
    
    tech_skills = [
        'python', 'aws', 'docker', 'react', 'angular', 
        'node.js', 'postgresql', 'mysql', 'mongodb', 'git',
        'jenkins', 'linux', 'rest api', 'graphql',
        'typescript', 'sql',
        'nosql', 'ci/cd'
    ]
    
    skill_counts = Counter()
    for skill in tech_skills:
        pattern = r'(?<!\w)' + re.escape(skill) + r'(?!\w)'
        matches = re.findall(pattern, cv_text)
        if matches:
            skill_counts[skill] = len(matches)
    
    max_count = max(skill_counts.values(), default=1)
    return {skill.capitalize(): min(round((count/max_count)*3), 3) 
            for skill, count in skill_counts.items()}

def parse_job_date(date_str):
    if not date_str:
        return datetime.min
    try:
        parsed = parse(date_str, settings={'RELATIVE_BASE': datetime.now()})
        return parsed if parsed else datetime.min
    except:
        return datetime.min

def calculate_match_score(description):
    score = 0
    desc_lower = (description or "").lower()
    for skill, weight in RESUME_SKILLS.items():
        if re.search(r'\b' + re.escape(skill.lower()) + r'\b', desc_lower):
            score += weight
    return score

def extract_salary(text):
    patterns = [
        r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)\s*LPA',
        r'â‚¹?\s*(\d+\.?\d*)\s*-\s*(\d+\.?\d*)\s*lakhs?'
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return float(match.group(2))
    return 0

def load_previous_jobs():
    """Load previously seen jobs from history file"""
    seen_jobs = {}
    if os.path.exists(HISTORY_CSV):
        with open(HISTORY_CSV, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                key = f"{row['company']}-{row['title']}-{row['url']}"
                seen_jobs[key] = {
                    'last_seen': datetime.strptime(row['last_seen'], "%Y-%m-%d"),
                    'times_seen': int(row['times_seen']),
                    'best_score': float(row['best_score'])
                }
    return seen_jobs

def update_job_history(jobs):
    """Update job history with current scrape results"""
    fieldnames = ['company', 'title', 'url', 'last_seen', 'times_seen', 'best_score']
    
    history = load_previous_jobs()
    
    for job in jobs:
        key = f"{job['company']}-{job['title']}-{job['url']}"
        score = job['match_score']
        
        if key in history:
            history[key]['times_seen'] += 1
            history[key]['last_seen'] = datetime.now()
            if score > history[key]['best_score']:
                history[key]['best_score'] = score
        else:
            history[key] = {
                'last_seen': datetime.now(),
                'times_seen': 1,
                'best_score': score
            }
    
    with open(HISTORY_CSV, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for key, data in history.items():
            company, title, url = key.split('-', 2)
            writer.writerow({
                'company': company,
                'title': title,
                'url': url,
                'last_seen': data['last_seen'].strftime("%Y-%m-%d"),
                'times_seen': data['times_seen'],
                'best_score': data['best_score']
            })

def process_linkedin_card(card):
    driver = None
    try:
        with CHROME_SEMAPHORE:
            service = Service(executable_path=CHROME_DRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=chrome_options)
            
            title_elem = card.find_element(By.CSS_SELECTOR, "h3.base-search-card__title")
            company_elem = card.find_element(By.CSS_SELECTOR, "h4.base-search-card__subtitle")
            location_elem = card.find_element(By.CSS_SELECTOR, "span.job-search-card__location")
            link_elem = card.find_element(By.CSS_SELECTOR, "a.base-card__full-link")
            date_elem = card.find_element(By.CSS_SELECTOR, "time")
            
            job_data = {
                "source": "linkedin",
                "title": title_elem.text.strip(),
                "company": company_elem.text.strip(),
                "location": location_elem.text.strip(),
                "url": link_elem.get_attribute("href"),
                "posted_date": parse_job_date(date_elem.get_attribute("datetime")),
            }
            
            driver.get(job_data["url"])
            try:
                WebDriverWait(driver, 7).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, ".show-more-less-html__markup"))
                )
                job_data["description"] = driver.find_element(By.CSS_SELECTOR, ".show-more-less-html__markup").text
            except:
                job_data["description"] = ""
            
            return job_data
    except Exception as e:
        return None
    finally:
        if driver:
            driver.quit()

def scrape_linkedin_title_location(args):
    """Scrape LinkedIn jobs with timeout handling"""
    title, location = args
    service = Service(executable_path=CHROME_DRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=chrome_options)
    jobs = []
    
    try:
        for page in range(MAX_LINKEDIN_PAGES):
            url = f"https://www.linkedin.com/jobs/search/?keywords={title}&location={location}&start={page*25}"
            driver.get(url)
            time.sleep(2)
            
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.jobs-search__results-list"))
                )
            except:
                continue
            
            job_cards = driver.find_elements(By.CSS_SELECTOR, "ul.jobs-search__results-list li")[:10]
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                for card in job_cards:
                    futures.append(executor.submit(process_linkedin_card, card))
                
                for future in concurrent.futures.as_completed(futures):
                    try:
                        result = future.result(timeout=20)
                        if result:
                            jobs.append(result)
                    except:
                        continue
    finally:
        driver.quit()
    return jobs

def scrape_company_page_dynamic(company):
    """Dynamic company scraper using flexible patterns"""
    jobs = []
    service = Service(executable_path=CHROME_DRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        scraper = DynamicScraper(driver)
        scraped_jobs = scraper.scrape(company["career_url"])
        
        for job in scraped_jobs[:MAX_COMPANY_JOBS]:
            jobs.append({
                "source": "company",
                "title": job['title'],
                "company": company["name"],
                "location": job['location'],
                "url": job['url'],
                "posted_date": datetime.now(),
                "description": job['title']
            })
    except Exception as e:
        logging.error(f"Failed to scrape {company['name']}: {str(e)}")
    finally:
        driver.quit()
    
    return jobs

def filter_and_sort_jobs(jobs):
    """Enhanced sorting with historical priority"""
    history = load_previous_jobs()
    
    processed = []
    for job in jobs:
        key = f"{job['company']}-{job['title']}-{job['url']}"
        job['historical_data'] = history.get(key, None)
        
        priority_boost = 0
        if job['historical_data']:
            priority_boost = job['historical_data']['times_seen'] * 0.5
            priority_boost += job['historical_data']['best_score'] * 0.3
            
        job['match_score'] = calculate_match_score(job.get('description', ''))
        job['salary_value'] = extract_salary(job.get('description', ''))
        
        processed.append({
            **job,
            'priority_score': job['match_score'] + priority_boost
        })
    
    recent_jobs = [j for j in processed if (datetime.now() - j["posted_date"]).days <= 7]
    filtered = [j for j in recent_jobs if not j['historical_data'] or j['historical_data']['best_score'] < j['match_score']]
    
    sorted_jobs = sorted(filtered, key=lambda x: (
        -x["priority_score"],
        -x["posted_date"].timestamp()
    ))
    
    return sorted_jobs[:MAX_JOBS]

def save_jobs_to_csv(jobs):
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Posted Date", "Company", "Title", "Location", "Match Score", "Salary (LPA)", "URL"])
        for job in jobs:
            writer.writerow([
                job["posted_date"].strftime("%Y-%m-%d"),
                job["company"],
                job["title"],
                job["location"],
                job["match_score"],
                f"{job['salary_value']:.1f}" if job["salary_value"] else "N/A",
                job["url"]
            ])

def main():
    global RESUME_SKILLS
    start_time = time.time()
    
    RESUME_SKILLS = extract_resume_skills()
    print("âœ… Resume Skills Detected:")
    for skill, weight in RESUME_SKILLS.items():
        print(f" - {skill}: {weight}/3")
    
    companies = pd.read_csv(COMPANY_LIST_PATH).head(20).to_dict("records")
    
    progress = tqdm(desc="Scraping Progress", unit="job")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        linkedin_args = [(title, loc) for title in JOB_TITLES for loc in LOCATION_PRIORITY]
        linkedin_future = {executor.submit(scrape_linkedin_title_location, arg): arg for arg in linkedin_args}
        
        company_future = {executor.submit(scrape_company_page_dynamic, comp): comp for comp in companies}
        
        linkedin_jobs = []
        company_jobs = []
        
        for future in concurrent.futures.as_completed(linkedin_future):
            result = future.result()
            linkedin_jobs.extend(result)
            progress.update(len(result))
        
        for future in concurrent.futures.as_completed(company_future):
            result = future.result()
            company_jobs.extend(result)
            progress.update(len(result))
    
    progress.close()
    
    all_jobs = linkedin_jobs + company_jobs
    sorted_jobs = filter_and_sort_jobs(all_jobs)
    
    update_job_history(all_jobs)
    save_jobs_to_csv(sorted_jobs)
    
    print(f"\nðŸŽ¯ Found {len(sorted_jobs)}/{len(all_jobs)} recent jobs matching criteria")
    print(f"â±ï¸  Execution time: {(time.time()-start_time)/60:.1f} minutes")
    print(f"ðŸ’¾ Saved to {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
